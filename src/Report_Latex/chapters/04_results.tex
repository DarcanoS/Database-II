\chapter{Results}
\label{ch:results}

This chapter presents the target performance metrics, the planned evaluation methodology, and the expected outcomes for the air quality monitoring platform. Because the project is under active development, this chapter documents the \textit{expected results} and validation framework that will be executed once the implementation phase is complete.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Target Performance Metrics}
\label{sec:target_metrics}

Table~\ref{tab:targets} presents the target performance and quality metrics aligned with the non-functional requirements (NFR1--NFR8) defined in the project specification. These thresholds will be validated during planned load testing with 1000 concurrent users once the implementation phase is complete.

\begin{table}[tb]
\centering
\caption{Target performance and quality metrics mapped to non-functional requirements.}
\label{tab:targets}
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Target (p95)} & \textbf{Requirement}\\
\midrule
Query latency ($\geq$1M rows)      & $\leq2$ s        & NFR1, NFR3\\
Dashboard load time           & $\leq2$ s        & NFR6, US12\\
Report generation             & $\leq10$ s       & NFR4\\
Recommendation update freq.   & $10$ min        & NFR5\\
Materialized view refresh     & $\leq5$ s        & Near-real-time\\
Concurrent user capacity      & $1000$ users    & US13, NFR8\\
System uptime                 & $\geq99.9\%$     & NFR7, US14\\
Peak CPU utilization          & $<70\%$         & Headroom\\
\bottomrule
\end{tabular}
\end{table}

The metrics in Table~\ref{tab:targets} cover multiple dimensions:
\begin{itemize}
    \item \textbf{Query performance}: sub-2-second response times at the 95th percentile over partitioned hypertables with $\geq$1 million air quality records, ensuring acceptable end-user experience even under heavy load (NFR1, NFR3).
    \item \textbf{Dashboard responsiveness}: complete dashboard rendering (including API calls, data aggregation, and visualization) within 2 seconds to meet citizen expectations for near-real-time monitoring (NFR6).
    \item \textbf{Report generation}: CSV export and summary report creation in under 10 seconds for researcher data access workflows (NFR4).
    \item \textbf{Data freshness}: 10-minute update cycle from external APIs (AQICN, Google, IQAir) to ensure recommendations reflect current air quality conditions (NFR5).
    \item \textbf{View maintenance}: materialized view and continuous aggregate refresh in under 5 seconds to provide low-latency pre-computed aggregations.
    \item \textbf{Scalability}: support for 1000 concurrent users without exceeding 70\% peak CPU utilization, leaving headroom for traffic spikes (NFR8).
    \item \textbf{Availability}: $\geq$99.9\% uptime validated through fault tolerance and failover tests (NFR7).
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Evaluation Methodology}
\label{sec:evaluation_methodology}

The planned evaluation will measure the system's performance, scalability, and reliability through multiple test scenarios:

\subsection{Query Performance Testing}
Execution time measurements will be collected over partitioned TimescaleDB hypertables containing $\geq$1 million air quality records. Test queries will include:
\begin{itemize}
    \item Time-range filters: retrieve all pollutant readings for Bogotá in a given month.
    \item Spatial filters: find all stations within a geographic bounding box.
    \item Compound filters: query PM2.5 values exceeding AQI threshold 151 in the last 7 days.
\end{itemize}
Each query type will be executed under varying concurrency levels (10, 100, 500, 1000 users) and latency distributions (p50, p95, p99) will be recorded using Prometheus metrics and \texttt{pg\_stat\_statements}.

\subsection{Dashboard and API Responsiveness}
End-to-end latency from HTTP request to dashboard rendering will be measured under normal and peak load conditions. This includes:
\begin{itemize}
    \item REST API call latency (JSON serialization overhead).
    \item GraphQL nested query resolution time.
    \item Front-end rendering and visualization loading (Grafana/custom dashboards).
\end{itemize}

\subsection{Scalability and Load Testing}
Apache JMeter scripts will simulate 1000 concurrent users accessing dashboards, generating CSV reports, and triggering personalized recommendations simultaneously. Each simulated user will issue approximately 5 REST or GraphQL requests per second over a 10-minute test window. Metrics collected:
\begin{itemize}
    \item Request throughput (requests/second).
    \item Error rate (HTTP 5xx responses).
    \item CPU, memory, and disk I/O utilization on database and API nodes.
    \item Connection pool saturation and query queue depth.
\end{itemize}

\subsection{Data Freshness and Ingestion Lag}
Monitoring of ingestion-to-availability lag for the 10-minute update cycle from external APIs. Measurements will include:
\begin{itemize}
    \item Poll latency: time to retrieve JSON payloads from AQICN, Google, and IQAir.
    \item Normalization latency: field mapping, unit conversion, and insertion into TimescaleDB.
    \item Materialized view refresh latency: time to update pre-aggregated views after new data arrival.
\end{itemize}

\subsection{Availability and Fault Tolerance}
Uptime tracking and fault tolerance validation through simulated node failures:
\begin{itemize}
    \item Database failover: simulate primary node failure and measure recovery time.
    \item API node failure: validate load balancer rerouting and session preservation.
    \item Network partition: test behavior under split-brain scenarios and quorum loss.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Expected Outcomes}
\label{sec:expected_outcomes}

Once implementation and load testing are complete, we expect to validate:
\begin{enumerate}
    \item Sub-2-second query latency at p95 for datasets exceeding 1 million rows under 1000 concurrent users (NFR1, US13).
    \item 10-minute recommendation refresh cycles aligned with WHO health guidelines and external API update intervals (NFR5, US8).
    \item System uptime $\geq$99.9\% with geographic redundancy and automated failover (NFR7, US14).
    \item Peak CPU utilization below 70\% under maximum planned load, leaving operational headroom for traffic spikes.
    \item Successful ingestion and storage of three years (2022--2024) of Bogotá air quality data, with potential expansion to 2018--2024 if storage and performance targets are met.
\end{enumerate}

Future iterations of this report will present measured results including:
\begin{itemize}
    \item Throughput-versus-concurrency curves showing system behavior under increasing load.
    \item Ingestion lag distribution over 24-hour periods during normal and peak API availability.
    \item Comparison against baseline PostgreSQL performance without TimescaleDB optimizations to quantify hypertable and continuous aggregate benefits.
    \item Error rate and recovery time distributions under simulated failure scenarios.
\end{itemize}



